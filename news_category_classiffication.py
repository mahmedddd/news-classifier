# -*- coding: utf-8 -*-
"""News Category_Classiffication.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OuZJf0_bkrtq1eV7UJAAafjyYM9ASqMl
"""

# Commented out IPython magic to ensure Python compatibility.
import os
import textwrap
import shutil

repo_path = "/content/news-classifier"

# Change directory to /content (or anywhere safe) before deleting
os.chdir("/content")

if os.path.exists(repo_path):
    shutil.rmtree(repo_path)
    print(f"Deleted existing folder: {repo_path}")

from getpass import getpass

username = "mahmedddd"
repo_name = "news-classifier"
token = getpass("Enter your GitHub token: ")

repo_url = f"https://{token}@github.com/{username}/{repo_name}.git"

# Now clone fresh
!git clone $repo_url



repo_path = f"/content/{repo_name}"
os.makedirs(f"{repo_path}/data", exist_ok=True)

# Move the IMDB dataset into repo's data directory
!cp "/content/AG News Classification Dataset.zip" "{repo_path}/data/AG News Classification Dataset.zip"


readme_content = textwrap.dedent("""
   # AG News Classification

This project performs news article classification on the AG News dataset using multiple approaches including classical machine learning (Logistic Regression, Random Forest) and a feedforward neural network built with TensorFlow/Keras.

## Dataset

The AG News Classification Dataset contains news articles with labels: World, Sports, Business, and Sci/Tech.

## Features

- Data preprocessing with NLTK (stopword removal, lemmatization)
- Use of pretrained GloVe embeddings for text representation
- Training and evaluation of Logistic Regression and Random Forest classifiers
- Building and training a feedforward neural network using TensorFlow/Keras
- Visualization of top words and word clouds for each category

## Requirements

Install the required libraries using:

```bash
pip install pandas numpy scikit-learn matplotlib seaborn nltk wordcloud tensorflow

""")

with open(f"{repo_path}/README.md", "w") as f:
    f.write(readme_content)

# Add, commit, and push repo changes
# %cd {repo_path}
!git config --global user.email "ahmedunited902@gmail.com"
!git config --global user.name "mahmedddd"

!git add .
!git commit -m "Initial commit with README and dataset included"
!git push origin main

"""### Download and unzip dataset, check files"""

!wget https://raw.githubusercontent.com/mahmedddd/news-classifier/refs/heads/main/data/AG%20News%20Classification%20Dataset.zip -O ag_news.zip

# Unzip the downloaded file
import zipfile

with zipfile.ZipFile("ag_news.zip", "r") as zip_ref:
    zip_ref.extractall("AG News Dataset")

# Check the contents
import os
print(os.listdir("AG News Dataset"))

"""### Load CSV files into pandas DataFrames"""

import pandas as pd

train_df = pd.read_csv('AG News Dataset/train.csv', header=None, names=['label', 'title', 'description'])
test_df = pd.read_csv('AG News Dataset/test.csv', header=None, names=['label', 'title', 'description'])

# title + description into a single text column
train_df['text'] = train_df['title'] + " " + train_df['description']
test_df['text'] = test_df['title'] + " " + test_df['description']

# labels to be 0-indexed (0 = World, 1 = Sports, 2 = Business, 3 = Sci/Tech)

train_df['label'] = pd.to_numeric(train_df['label'], errors='coerce').fillna(0).astype(int) - 1
test_df['label'] = pd.to_numeric(test_df['label'], errors='coerce').fillna(0).astype(int) - 1

# Remove rows with invalid/missing labels (label == -1)
train_df = train_df[train_df['label'] != -1]
test_df = test_df[test_df['label'] != -1]

# Confirm fix
print(train_df['label'].value_counts())
print(test_df['label'].value_counts())

"""### Download and import NLTK tools for text preprocessing"""

import nltk
import re
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer

# Download required NLTK resources (only once)
nltk.download('stopwords')
nltk.download('wordnet')

"""### Define preprocessing function and apply it to text data"""

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess(text):
    # Lowercase and remove non-letter characters
    text = re.sub(r'[^a-zA-Z\s]', '', text.lower())
    # Tokenize and lemmatize, remove stopwords
    tokens = text.split()
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return ' '.join(tokens)

train_df.loc[:, 'clean_text'] = train_df['text'].apply(preprocess)
test_df.loc[:, 'clean_text'] = test_df['text'].apply(preprocess)

"""###  Download and extract GloVe pretrained embeddings"""

!wget http://nlp.stanford.edu/data/glove.6B.zip
!unzip glove.6B.zip -d glove

"""### Load GloVe embeddings into a dictionary"""

import numpy as np

embedding_index = {}
with open("glove/glove.6B.100d.txt", encoding='utf8') as f:
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embedding_index[word] = coefs

print(f"Loaded {len(embedding_index)} word vectors.")

"""### Function to get average embedding vector for a given text"""

def get_embedding_vector(text):
    words = text.split()
    vectors = [embedding_index[word] for word in words if word in embedding_index]
    if len(vectors) == 0:
        return np.zeros(100)
    return np.mean(vectors, axis=0)

# Convert text to average GloVe embeddings
X_train = np.vstack(train_df['clean_text'].apply(get_embedding_vector))
X_test = np.vstack(test_df['clean_text'].apply(get_embedding_vector))

# Labels
y_train = train_df['label']
y_test = test_df['label']

"""### Train and evaluate Logistic Regression classifier"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report

clf = LogisticRegression(max_iter=1000)
clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)
print(classification_report(y_test, y_pred, target_names=["World", "Sports", "Business", "Sci/Tech"]))

"""###Train and evaluate Random Forest classifier"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred_rf))

"""# **BONUS TASKS**

### Install wordcloud library for visualization
"""

!pip install wordcloud

"""### Visualize most frequent words per category using bar plots and word clouds"""

from collections import Counter
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud

def get_top_words(texts, n=20):
    all_words = ' '.join(texts).split()
    return Counter(all_words).most_common(n)

# Map label to category names
label_map = {0: "World", 1: "Sports", 2: "Business", 3: "Sci/Tech"}

for label, category in label_map.items():
    subset = train_df[train_df['label'] == label]
    top_words = get_top_words(subset['clean_text'], n=15)
    words, freqs = zip(*top_words)

    plt.figure(figsize=(10, 5))
    sns.barplot(x=list(freqs), y=list(words), palette="viridis")
    plt.title(f"Top Words in {category} News")
    plt.xlabel("Frequency")
    plt.ylabel("Words")
    plt.show()

for label, category in label_map.items():
    text = ' '.join(train_df[train_df['label'] == label]['clean_text'])
    wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)

    plt.figure(figsize=(10, 5))
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.title(f"Word Cloud for {category} News")
    plt.show()

"""### Install TensorFlow to build neural network"""

!pip install tensorflow

"""### Build, compile and train a feedforward neural network using Keras"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.utils import to_categorical

# Convert labels to one-hot encoding
y_train_cat = to_categorical(y_train, num_classes=4)
y_test_cat = to_categorical(y_test, num_classes=4)

from keras.models import Sequential
from keras.layers import Dense, Dropout, Input

# Build model
model = Sequential([
    Input(shape=(X_train.shape[1],)),     # ðŸ‘ˆ Explicit input layer
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(64, activation='relu'),
    Dropout(0.3),
    Dense(4, activation='softmax')        # 4 classes
])

# Compile model
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train
history = model.fit(X_train, y_train_cat, epochs=10, batch_size=64,
                    validation_data=(X_test, y_test_cat))

"""### Evaluate neural network and plot training history"""

loss, acc = model.evaluate(X_test, y_test_cat)
print(f"Neural Network Accuracy: {acc:.4f}")

import matplotlib.pyplot as plt

# Accuracy
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()